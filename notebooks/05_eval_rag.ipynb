{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dspy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.chromadb_rm import ChromadbRM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateAnswer(dspy.Signature):\n",
    "    \"\"\"Answer questions given the context\"\"\"\n",
    "\n",
    "    context = dspy.InputField(desc=\"may contain relevant facts\")\n",
    "    question = dspy.InputField()\n",
    "    answer = dspy.OutputField(desc=\"Short factual answer to the question. 1 - 5 words long.\")\n",
    "\n",
    "class RAG(dspy.Module):\n",
    "    def __init__(self, num_passages=5):\n",
    "        super().__init__()\n",
    "        self.retrieve = dspy.Retrieve(k=num_passages)\n",
    "        self.generate_answer = dspy.ChainOfThought(GenerateAnswer)\n",
    "    \n",
    "    def forward(self, question):\n",
    "        context = self.retrieve(question).passages\n",
    "        prediction = self.generate_answer(context=context, question=question)\n",
    "        return dspy.Prediction(context=context, answer=prediction.answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup():\n",
    "    \"\"\"\n",
    "    Setup the dsypy and retrieval models\n",
    "    \"\"\"\n",
    "\n",
    "    turbo = dspy.OpenAI(model='gpt-3.5-turbo')\n",
    "\n",
    "    chroma_rm = ChromadbRM(collection_name=\"test-overlap-0\", persist_directory=\"chroma.db\", local_embed_model=\"sentence-transformers/paraphrase-MiniLM-L6-v2\",\n",
    "                                   openai_api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "    dspy.settings.configure(lm=turbo, rm=chroma_rm)\n",
    "    \n",
    "    rag = RAG()\n",
    "\n",
    "    return rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection Count: 7850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sudhanva/miniconda3/lib/python3.11/site-packages/torch/cuda/__init__.py:138: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 804: forward compatibility was attempted on non supported HW (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "rag = setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read question, ground_truths from ./data/processed/synthetic_dataset.csv\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"./data/processed/synthetic_dataset.csv\")\n",
    "\n",
    "df = df[['question', 'ground_truths']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>ground_truths</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Who directed the play \"How to Curse\" in 2007?</td>\n",
       "      <td>['Josie Rourke']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Who directed the film \"Donkey Punch\"?</td>\n",
       "      <td>['Olly Blackburn.']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Who was Du Fu's paternal grandfather?</td>\n",
       "      <td>['Du Shenyan.']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How many children did Du Fu have by 757?</td>\n",
       "      <td>['Five.']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Where did Du Fu spend most of the next five ye...</td>\n",
       "      <td>['Sichuan province.']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question          ground_truths\n",
       "0      Who directed the play \"How to Curse\" in 2007?       ['Josie Rourke']\n",
       "1              Who directed the film \"Donkey Punch\"?    ['Olly Blackburn.']\n",
       "2              Who was Du Fu's paternal grandfather?        ['Du Shenyan.']\n",
       "3           How many children did Du Fu have by 757?              ['Five.']\n",
       "4  Where did Du Fu spend most of the next five ye...  ['Sichuan province.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into train and test\n",
    "train, test = train_test_split(df, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the train and test data\n",
    "# train.to_csv(\"./data/processed/train_synthetic.csv\", index=False)\n",
    "# test.to_csv(\"./data/processed/test_synthetic.csv\", index=False)\n",
    "\n",
    "# load the train and test data\n",
    "train = pd.read_csv(\"./data/processed/train_synthetic.csv\")\n",
    "test = pd.read_csv(\"./data/processed/test_synthetic.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "# Create an empty list to store rows\n",
    "eval_results_rows = []\n",
    "\n",
    "for index, row in test.iterrows():\n",
    "    # Get the question\n",
    "    question = row['question']\n",
    "    # Response from rag\n",
    "    response = rag(question)\n",
    "    # Create a dictionary to represent a row\n",
    "    row_dict = {'question': question, 'contexts': response.context, 'answer': response.answer, 'ground_truths' : row['ground_truths']}\n",
    "    # Append the row dictionary to the list\n",
    "    eval_results_rows.append(row_dict)\n",
    "\n",
    "# Create the df_eval_results DataFrame from the list of rows\n",
    "df_eval_results = pd.DataFrame(eval_results_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>contexts</th>\n",
       "      <th>answer</th>\n",
       "      <th>ground_truths</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Who developed the Dvorak technique?</td>\n",
       "      <td>[= dvorak technique =, . development of the ob...</td>\n",
       "      <td>Vernon Dvorak</td>\n",
       "      <td>['Vernon Dvorak']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When did No. 20 Squadron relocate to Cairns?</td>\n",
       "      <td>[. now comprising 252 officers and men, the sq...</td>\n",
       "      <td>11 November 1942.</td>\n",
       "      <td>['11 November 1942.']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Who won the Claxton Shield's Most Valuable Pla...</td>\n",
       "      <td>[. at the baseball australia diamond awards, h...</td>\n",
       "      <td>Wayne Lundgren</td>\n",
       "      <td>['Wayne Lundgren.']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Q: Who did Lesnar attack after Rollins refused...</td>\n",
       "      <td>[. lesnar won the match and his third wwe cham...</td>\n",
       "      <td>The new day, the league of nations, and Kevin ...</td>\n",
       "      <td>['A: Booker T, John \"Bradshaw\" Layfield, Micha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Who is the ghost in the poem \"Little Gidding\"?</td>\n",
       "      <td>[. within the poem, the narrator meets a ghost...</td>\n",
       "      <td>Combination of poets and literary figures.</td>\n",
       "      <td>['Combination of literary figures.']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>Who did Brock Lesnar defeat to become the new ...</td>\n",
       "      <td>[. with the victory, lesnar became the undispu...</td>\n",
       "      <td>Shane Carwin</td>\n",
       "      <td>['Randy Couture.']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>When did Typhoon Kujira occur?</td>\n",
       "      <td>[. however, ultimately any effects in the arch...</td>\n",
       "      <td>April</td>\n",
       "      <td>['April.']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>What is the main sport in Manila?</td>\n",
       "      <td>[sports in manila have a long and distinguishe...</td>\n",
       "      <td>Basketball.</td>\n",
       "      <td>['Basketball.']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>Who ordered No. 202 Squadron RAF to Gibraltar?</td>\n",
       "      <td>[. so at 09 : 00 ( utc ) on the 9 september 19...</td>\n",
       "      <td>The order was given by an unspecified authority.</td>\n",
       "      <td>['Admiralty']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>What did soldiers build inside the Rock of Gib...</td>\n",
       "      <td>[work in gibraltar began immediately under com...</td>\n",
       "      <td>Tunnels and chambers.</td>\n",
       "      <td>['Tunnels and chambers.']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>86 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             question  \\\n",
       "0                 Who developed the Dvorak technique?   \n",
       "1        When did No. 20 Squadron relocate to Cairns?   \n",
       "2   Who won the Claxton Shield's Most Valuable Pla...   \n",
       "3   Q: Who did Lesnar attack after Rollins refused...   \n",
       "4      Who is the ghost in the poem \"Little Gidding\"?   \n",
       "..                                                ...   \n",
       "81  Who did Brock Lesnar defeat to become the new ...   \n",
       "82                     When did Typhoon Kujira occur?   \n",
       "83                  What is the main sport in Manila?   \n",
       "84     Who ordered No. 202 Squadron RAF to Gibraltar?   \n",
       "85  What did soldiers build inside the Rock of Gib...   \n",
       "\n",
       "                                             contexts  \\\n",
       "0   [= dvorak technique =, . development of the ob...   \n",
       "1   [. now comprising 252 officers and men, the sq...   \n",
       "2   [. at the baseball australia diamond awards, h...   \n",
       "3   [. lesnar won the match and his third wwe cham...   \n",
       "4   [. within the poem, the narrator meets a ghost...   \n",
       "..                                                ...   \n",
       "81  [. with the victory, lesnar became the undispu...   \n",
       "82  [. however, ultimately any effects in the arch...   \n",
       "83  [sports in manila have a long and distinguishe...   \n",
       "84  [. so at 09 : 00 ( utc ) on the 9 september 19...   \n",
       "85  [work in gibraltar began immediately under com...   \n",
       "\n",
       "                                               answer  \\\n",
       "0                                       Vernon Dvorak   \n",
       "1                                   11 November 1942.   \n",
       "2                                      Wayne Lundgren   \n",
       "3   The new day, the league of nations, and Kevin ...   \n",
       "4          Combination of poets and literary figures.   \n",
       "..                                                ...   \n",
       "81                                       Shane Carwin   \n",
       "82                                              April   \n",
       "83                                        Basketball.   \n",
       "84   The order was given by an unspecified authority.   \n",
       "85                              Tunnels and chambers.   \n",
       "\n",
       "                                        ground_truths  \n",
       "0                                   ['Vernon Dvorak']  \n",
       "1                               ['11 November 1942.']  \n",
       "2                                 ['Wayne Lundgren.']  \n",
       "3   ['A: Booker T, John \"Bradshaw\" Layfield, Micha...  \n",
       "4                ['Combination of literary figures.']  \n",
       "..                                                ...  \n",
       "81                                 ['Randy Couture.']  \n",
       "82                                         ['April.']  \n",
       "83                                    ['Basketball.']  \n",
       "84                                      ['Admiralty']  \n",
       "85                          ['Tunnels and chambers.']  \n",
       "\n",
       "[86 rows x 4 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "# df_eval_results ground_truths to list\n",
    "df_eval_results['ground_truths'] = df_eval_results['ground_truths'].apply(lambda x: ast.literal_eval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the df_eval_results DataFrame to a csv file\n",
    "import time\n",
    "EXP_NAME = \"SIMPLE_RAG_NO_OVERLAP\"\n",
    "TIMESTAMP = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "df_eval_results.to_csv('./results/inference_' + EXP_NAME + '_' + TIMESTAMP + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, that we have answers for all the questions, we can evaluate the RAG model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [context_precision]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:28<00:00,  4.83s/it]\n",
      "/home/sudhanva/miniconda3/lib/python3.11/site-packages/datasets/table.py:1395: FutureWarning: promote has been superseded by mode='default'.\n",
      "  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]\n",
      "/home/sudhanva/miniconda3/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [faithfulness]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:29<00:00,  5.00s/it]\n",
      "/home/sudhanva/miniconda3/lib/python3.11/site-packages/datasets/table.py:1395: FutureWarning: promote has been superseded by mode='default'.\n",
      "  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]\n",
      "/home/sudhanva/miniconda3/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [answer_relevancy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:52<00:00,  8.83s/it]\n",
      "/home/sudhanva/miniconda3/lib/python3.11/site-packages/datasets/table.py:1395: FutureWarning: promote has been superseded by mode='default'.\n",
      "  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]\n",
      "/home/sudhanva/miniconda3/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [context_recall]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:22<00:00,  3.69s/it]\n",
      "/home/sudhanva/miniconda3/lib/python3.11/site-packages/datasets/table.py:1395: FutureWarning: promote has been superseded by mode='default'.\n",
      "  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]\n",
      "/home/sudhanva/miniconda3/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [answer_similarity]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:03<00:00,  1.56it/s]\n",
      "/home/sudhanva/miniconda3/lib/python3.11/site-packages/datasets/table.py:1395: FutureWarning: promote has been superseded by mode='default'.\n",
      "  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]\n",
      "/home/sudhanva/miniconda3/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [context_relevancy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:12<00:00,  2.01s/it]\n",
      "/home/sudhanva/miniconda3/lib/python3.11/site-packages/datasets/table.py:1395: FutureWarning: promote has been superseded by mode='default'.\n",
      "  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]\n",
      "/home/sudhanva/miniconda3/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    }
   ],
   "source": [
    "from ragas.metrics import (\n",
    "    answer_relevancy,\n",
    "    faithfulness,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    "    answer_similarity,\n",
    "    context_relevancy\n",
    ")\n",
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "\n",
    "ds = Dataset.from_pandas(df_eval_results)\n",
    "\n",
    "result = evaluate(\n",
    "    ds,\n",
    "    metrics=[\n",
    "        context_precision,\n",
    "        faithfulness,\n",
    "        answer_relevancy,\n",
    "        context_recall,\n",
    "        answer_similarity,\n",
    "        context_relevancy\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context_precision': 0.5798, 'faithfulness': 0.6705, 'answer_relevancy': 0.8434, 'context_recall': 0.6988, 'answer_similarity': 0.8933, 'context_relevancy': 0.1628}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the result\n",
    "result.to_pandas().to_csv('./results/evaluation_' + EXP_NAME + '_' + TIMESTAMP + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sudhanva/miniconda3/lib/python3.11/site-packages/datasets/table.py:1395: FutureWarning: promote has been superseded by mode='default'.\n",
      "  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]\n",
      "/home/sudhanva/miniconda3/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>contexts</th>\n",
       "      <th>answer</th>\n",
       "      <th>ground_truths</th>\n",
       "      <th>context_precision</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>context_recall</th>\n",
       "      <th>answer_similarity</th>\n",
       "      <th>context_relevancy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Who developed the Dvorak technique?</td>\n",
       "      <td>[= dvorak technique =, . development of the ob...</td>\n",
       "      <td>Vernon Dvorak</td>\n",
       "      <td>[Vernon Dvorak]</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When did No. 20 Squadron relocate to Cairns?</td>\n",
       "      <td>[. now comprising 252 officers and men, the sq...</td>\n",
       "      <td>11 November 1942.</td>\n",
       "      <td>[11 November 1942.]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.952935</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.181818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Who won the Claxton Shield's Most Valuable Pla...</td>\n",
       "      <td>[. at the baseball australia diamond awards, h...</td>\n",
       "      <td>Wayne Lundgren</td>\n",
       "      <td>[Wayne Lundgren.]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.844966</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.985646</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Q: Who did Lesnar attack after Rollins refused...</td>\n",
       "      <td>[. lesnar won the match and his third wwe cham...</td>\n",
       "      <td>The new day, the league of nations, and Kevin ...</td>\n",
       "      <td>[A: Booker T, John \"Bradshaw\" Layfield, Michae...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.887968</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.792667</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Who is the ghost in the poem \"Little Gidding\"?</td>\n",
       "      <td>[. within the poem, the narrator meets a ghost...</td>\n",
       "      <td>Combination of poets and literary figures.</td>\n",
       "      <td>[Combination of literary figures.]</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.892179</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.958397</td>\n",
       "      <td>0.181818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>Who did Brock Lesnar defeat to become the new ...</td>\n",
       "      <td>[. with the victory, lesnar became the undispu...</td>\n",
       "      <td>Shane Carwin</td>\n",
       "      <td>[Randy Couture.]</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.982244</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.870798</td>\n",
       "      <td>0.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>When did Typhoon Kujira occur?</td>\n",
       "      <td>[. however, ultimately any effects in the arch...</td>\n",
       "      <td>April</td>\n",
       "      <td>[April.]</td>\n",
       "      <td>0.638889</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.926135</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.929672</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>What is the main sport in Manila?</td>\n",
       "      <td>[sports in manila have a long and distinguishe...</td>\n",
       "      <td>Basketball.</td>\n",
       "      <td>[Basketball.]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>Who ordered No. 202 Squadron RAF to Gibraltar?</td>\n",
       "      <td>[. so at 09 : 00 ( utc ) on the 9 september 19...</td>\n",
       "      <td>The order was given by an unspecified authority.</td>\n",
       "      <td>[Admiralty]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.987047</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.812652</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>What did soldiers build inside the Rock of Gib...</td>\n",
       "      <td>[work in gibraltar began immediately under com...</td>\n",
       "      <td>Tunnels and chambers.</td>\n",
       "      <td>[Tunnels and chambers.]</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.982333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>86 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             question  \\\n",
       "0                 Who developed the Dvorak technique?   \n",
       "1        When did No. 20 Squadron relocate to Cairns?   \n",
       "2   Who won the Claxton Shield's Most Valuable Pla...   \n",
       "3   Q: Who did Lesnar attack after Rollins refused...   \n",
       "4      Who is the ghost in the poem \"Little Gidding\"?   \n",
       "..                                                ...   \n",
       "81  Who did Brock Lesnar defeat to become the new ...   \n",
       "82                     When did Typhoon Kujira occur?   \n",
       "83                  What is the main sport in Manila?   \n",
       "84     Who ordered No. 202 Squadron RAF to Gibraltar?   \n",
       "85  What did soldiers build inside the Rock of Gib...   \n",
       "\n",
       "                                             contexts  \\\n",
       "0   [= dvorak technique =, . development of the ob...   \n",
       "1   [. now comprising 252 officers and men, the sq...   \n",
       "2   [. at the baseball australia diamond awards, h...   \n",
       "3   [. lesnar won the match and his third wwe cham...   \n",
       "4   [. within the poem, the narrator meets a ghost...   \n",
       "..                                                ...   \n",
       "81  [. with the victory, lesnar became the undispu...   \n",
       "82  [. however, ultimately any effects in the arch...   \n",
       "83  [sports in manila have a long and distinguishe...   \n",
       "84  [. so at 09 : 00 ( utc ) on the 9 september 19...   \n",
       "85  [work in gibraltar began immediately under com...   \n",
       "\n",
       "                                               answer  \\\n",
       "0                                       Vernon Dvorak   \n",
       "1                                   11 November 1942.   \n",
       "2                                      Wayne Lundgren   \n",
       "3   The new day, the league of nations, and Kevin ...   \n",
       "4          Combination of poets and literary figures.   \n",
       "..                                                ...   \n",
       "81                                       Shane Carwin   \n",
       "82                                              April   \n",
       "83                                        Basketball.   \n",
       "84   The order was given by an unspecified authority.   \n",
       "85                              Tunnels and chambers.   \n",
       "\n",
       "                                        ground_truths  context_precision  \\\n",
       "0                                     [Vernon Dvorak]           0.866667   \n",
       "1                                 [11 November 1942.]           1.000000   \n",
       "2                                   [Wayne Lundgren.]           1.000000   \n",
       "3   [A: Booker T, John \"Bradshaw\" Layfield, Michae...           0.000000   \n",
       "4                  [Combination of literary figures.]           0.200000   \n",
       "..                                                ...                ...   \n",
       "81                                   [Randy Couture.]           0.500000   \n",
       "82                                           [April.]           0.638889   \n",
       "83                                      [Basketball.]           1.000000   \n",
       "84                                        [Admiralty]           1.000000   \n",
       "85                            [Tunnels and chambers.]           0.500000   \n",
       "\n",
       "    faithfulness  answer_relevancy  context_recall  answer_similarity  \\\n",
       "0            1.0          0.999999             1.0           1.000000   \n",
       "1            1.0          0.952935             1.0           1.000000   \n",
       "2            1.0          0.844966             1.0           0.985646   \n",
       "3            0.0          0.887968             0.0           0.792667   \n",
       "4            1.0          0.892179             1.0           0.958397   \n",
       "..           ...               ...             ...                ...   \n",
       "81           1.0          0.982244             0.0           0.870798   \n",
       "82           1.0          0.926135             1.0           0.929672   \n",
       "83           1.0          1.000000             1.0           1.000000   \n",
       "84           0.5          0.987047             0.0           0.812652   \n",
       "85           1.0          0.982333             1.0           1.000000   \n",
       "\n",
       "    context_relevancy  \n",
       "0            0.125000  \n",
       "1            0.181818  \n",
       "2            0.000000  \n",
       "3            0.111111  \n",
       "4            0.181818  \n",
       "..                ...  \n",
       "81           0.428571  \n",
       "82           0.000000  \n",
       "83           0.300000  \n",
       "84           0.200000  \n",
       "85           0.111111  \n",
       "\n",
       "[86 rows x 10 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mprasadshreyas\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sudhanva/code/takehome/wandb/run-20240129_182612-sjd56jgz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/prasadshreyas/wikitext-rag-eval/runs/sjd56jgz' target=\"_blank\">dulcet-shadow-6</a></strong> to <a href='https://wandb.ai/prasadshreyas/wikitext-rag-eval' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/prasadshreyas/wikitext-rag-eval' target=\"_blank\">https://wandb.ai/prasadshreyas/wikitext-rag-eval</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/prasadshreyas/wikitext-rag-eval/runs/sjd56jgz' target=\"_blank\">https://wandb.ai/prasadshreyas/wikitext-rag-eval/runs/sjd56jgz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa86ff5a2e1b491d93638aecd020d803",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.020 MB of 0.020 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>answer_relevancy</td><td>▁</td></tr><tr><td>answer_similarity</td><td>▁</td></tr><tr><td>context_precision</td><td>▁</td></tr><tr><td>context_recall</td><td>▁</td></tr><tr><td>context_relevancy</td><td>▁</td></tr><tr><td>faithfulness</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>answer_relevancy</td><td>0.84338</td></tr><tr><td>answer_similarity</td><td>0.89332</td></tr><tr><td>context_precision</td><td>0.57984</td></tr><tr><td>context_recall</td><td>0.69884</td></tr><tr><td>context_relevancy</td><td>0.16279</td></tr><tr><td>faithfulness</td><td>0.67054</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dulcet-shadow-6</strong> at: <a href='https://wandb.ai/prasadshreyas/wikitext-rag-eval/runs/sjd56jgz' target=\"_blank\">https://wandb.ai/prasadshreyas/wikitext-rag-eval/runs/sjd56jgz</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240129_182612-sjd56jgz/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Logging to wandb\n",
    "\n",
    "import wandb\n",
    "\n",
    "# start a new wandb run to track this script\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"wikitext-rag-eval\",\n",
    "    \n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "        \"number_of_questions\": len(ds),\n",
    "        \"comments\": \"Simple QA RAG model with no teleprompter - chunk overlap size 0\",\n",
    "        \"model\": \"RAG\",\n",
    "        \"dataset\": \"Synthetic\",\n",
    "        \"num_passages\": 5,\n",
    "        \"openai_model\": \"gpt-3.5-turbo\",\n",
    "        \"chroma_collection_name\": \"test-overlap-64\",\n",
    "        \"chroma_persist_directory\": \"chroma.db\",\n",
    "        \"chroma_local_embed_model\": \"sentence-transformers/paraphrase-MiniLM-L6-v2\",\n",
    "\n",
    "    }\n",
    ")\n",
    "\n",
    "wandb.log(result)\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's compile the RAG using teleprompters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>ground_truths</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Who was the youngest Cy Young winner before th...</td>\n",
       "      <td>['Dwight Gooden']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How many crewmen were estimated to be wounded ...</td>\n",
       "      <td>['111-121 crewmen.']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What were the design faults of the Tetrarch tank?</td>\n",
       "      <td>['Size limitation, crew shortage, cooling syst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What was the maximum speed of the Mk VII tank?</td>\n",
       "      <td>['40 miles per hour.']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the distinguishing feature of metoposa...</td>\n",
       "      <td>['Positioning of eye sockets.']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>How do temnospondyls change during metamorphosis?</td>\n",
       "      <td>['Reshaping and strengthening of bones.']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Who directed the film \"The Heart of Ezra Greer\"?</td>\n",
       "      <td>['Emile Chautard']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Who owned the Spanish villa used as a base by ...</td>\n",
       "      <td>['Italian officer']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Where is San Lorenzo Colossal Head 2 currently...</td>\n",
       "      <td>['Mexico City.']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Q: What was the mission of the US 2nd Division...</td>\n",
       "      <td>['A: Destroy North Koreans and restore river l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  Who was the youngest Cy Young winner before th...   \n",
       "1  How many crewmen were estimated to be wounded ...   \n",
       "2  What were the design faults of the Tetrarch tank?   \n",
       "3     What was the maximum speed of the Mk VII tank?   \n",
       "4  What is the distinguishing feature of metoposa...   \n",
       "5  How do temnospondyls change during metamorphosis?   \n",
       "6   Who directed the film \"The Heart of Ezra Greer\"?   \n",
       "7  Who owned the Spanish villa used as a base by ...   \n",
       "8  Where is San Lorenzo Colossal Head 2 currently...   \n",
       "9  Q: What was the mission of the US 2nd Division...   \n",
       "\n",
       "                                       ground_truths  \n",
       "0                                  ['Dwight Gooden']  \n",
       "1                               ['111-121 crewmen.']  \n",
       "2  ['Size limitation, crew shortage, cooling syst...  \n",
       "3                             ['40 miles per hour.']  \n",
       "4                    ['Positioning of eye sockets.']  \n",
       "5          ['Reshaping and strengthening of bones.']  \n",
       "6                                 ['Emile Chautard']  \n",
       "7                                ['Italian officer']  \n",
       "8                                   ['Mexico City.']  \n",
       "9  ['A: Destroy North Koreans and restore river l...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "trainset = []\n",
    "for i in range(5):\n",
    "    ex = dspy.Example(\n",
    "        question=train['question'].iloc[i],\n",
    "        answer=ast.literal_eval(train['ground_truths'].iloc[i])[0]\n",
    "    )\n",
    "    ex = ex.with_inputs('question')\n",
    "    trainset.append(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Example({'question': 'Who was the youngest Cy Young winner before the person mentioned in the context?', 'answer': 'Dwight Gooden'}) (input_keys={'question'}),\n",
       " Example({'question': 'How many crewmen were estimated to be wounded during the attack on Hyūga?', 'answer': '111-121 crewmen.'}) (input_keys={'question'}),\n",
       " Example({'question': 'What were the design faults of the Tetrarch tank?', 'answer': 'Size limitation, crew shortage, cooling system.'}) (input_keys={'question'}),\n",
       " Example({'question': 'What was the maximum speed of the Mk VII tank?', 'answer': '40 miles per hour.'}) (input_keys={'question'}),\n",
       " Example({'question': 'What is the distinguishing feature of metoposaurids compared to capitosauroids?', 'answer': 'Positioning of eye sockets.'}) (input_keys={'question'})]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:05<00:00,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 5 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from dspy.teleprompt import BootstrapFewShot\n",
    "\n",
    "# Validation logic: check that the predicted answer is correct.\n",
    "# Also check that the retrieved context does actually contain that answer.\n",
    "def validate_context_and_answer(example, pred, trace=None):\n",
    "    answer_EM = dspy.evaluate.answer_exact_match(example, pred)\n",
    "    answer_PM = dspy.evaluate.answer_passage_match(example, pred)\n",
    "    return answer_EM and answer_PM\n",
    "\n",
    "# Set up a basic teleprompter, which will compile our RAG program.\n",
    "teleprompter = BootstrapFewShot(metric=validate_context_and_answer)\n",
    "\n",
    "# Compile!\n",
    "compiled_rag = teleprompter.compile(RAG(), trainset=trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "def get_evals(dataset, rag):\n",
    "    # Create an empty list to store rows\n",
    "    eval_results_rows = []\n",
    "\n",
    "    for index, row in dataset.iterrows():\n",
    "        # Get the question\n",
    "        question = row['question']\n",
    "        # Response from rag\n",
    "        response = rag(question)\n",
    "        # Create a dictionary to represent a row\n",
    "        row_dict = {'question': question, 'contexts': response.context, 'answer': response.answer, 'ground_truths' : row['ground_truths']}\n",
    "        # Append the row dictionary to the list\n",
    "        eval_results_rows.append(row_dict)\n",
    "\n",
    "    # Create the df_eval_results DataFrame from the list of rows\n",
    "    df_eval_results = pd.DataFrame(eval_results_rows)\n",
    "\n",
    "    # Convert 'ground_truths' column to list\n",
    "    df_eval_results['ground_truths'] = df_eval_results['ground_truths'].apply(lambda x: ast.literal_eval(x))\n",
    "\n",
    "    return df_eval_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_eval_results = get_evals(test, compiled_rag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the df_eval_results DataFrame to a csv file\n",
    "import time\n",
    "EXP_NAME = \"COMPILED_RAG_OVERLAP_0\"\n",
    "TIMESTAMP = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "df_eval_results.to_csv('./results/inference_' + EXP_NAME + '_' + TIMESTAMP + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, that we have answers for all the questions, we can evaluate the RAG model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [context_precision]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:19<00:00,  3.27s/it]\n",
      "/home/sudhanva/miniconda3/lib/python3.11/site-packages/datasets/table.py:1395: FutureWarning: promote has been superseded by mode='default'.\n",
      "  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]\n",
      "/home/sudhanva/miniconda3/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [faithfulness]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:29<00:00,  4.97s/it]\n",
      "/home/sudhanva/miniconda3/lib/python3.11/site-packages/datasets/table.py:1395: FutureWarning: promote has been superseded by mode='default'.\n",
      "  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]\n",
      "/home/sudhanva/miniconda3/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [answer_relevancy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:48<00:00,  8.14s/it]\n",
      "/home/sudhanva/miniconda3/lib/python3.11/site-packages/datasets/table.py:1395: FutureWarning: promote has been superseded by mode='default'.\n",
      "  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]\n",
      "/home/sudhanva/miniconda3/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [context_recall]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:19<00:00,  3.19s/it]\n",
      "/home/sudhanva/miniconda3/lib/python3.11/site-packages/datasets/table.py:1395: FutureWarning: promote has been superseded by mode='default'.\n",
      "  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]\n",
      "/home/sudhanva/miniconda3/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [answer_similarity]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:05<00:00,  1.08it/s]\n",
      "/home/sudhanva/miniconda3/lib/python3.11/site-packages/datasets/table.py:1395: FutureWarning: promote has been superseded by mode='default'.\n",
      "  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]\n",
      "/home/sudhanva/miniconda3/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [context_relevancy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:12<00:00,  2.02s/it]\n",
      "/home/sudhanva/miniconda3/lib/python3.11/site-packages/datasets/table.py:1395: FutureWarning: promote has been superseded by mode='default'.\n",
      "  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]\n",
      "/home/sudhanva/miniconda3/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    }
   ],
   "source": [
    "ds = Dataset.from_pandas(df_eval_results)\n",
    "\n",
    "result = evaluate(\n",
    "    ds,\n",
    "    metrics=[\n",
    "        context_precision,\n",
    "        faithfulness,\n",
    "        answer_relevancy,\n",
    "        context_recall,\n",
    "        answer_similarity,\n",
    "        context_relevancy\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context_precision': 0.5776, 'faithfulness': 0.6512, 'answer_relevancy': 0.8535, 'context_recall': 0.6734, 'answer_similarity': 0.8925, 'context_relevancy': 0.1689}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the result\n",
    "result.to_pandas().to_csv('./results/evaluation_' + EXP_NAME + '_' + TIMESTAMP + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sudhanva/miniconda3/lib/python3.11/site-packages/datasets/table.py:1395: FutureWarning: promote has been superseded by mode='default'.\n",
      "  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]\n",
      "/home/sudhanva/miniconda3/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>contexts</th>\n",
       "      <th>answer</th>\n",
       "      <th>ground_truths</th>\n",
       "      <th>context_precision</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>context_recall</th>\n",
       "      <th>answer_similarity</th>\n",
       "      <th>context_relevancy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Who developed the Dvorak technique?</td>\n",
       "      <td>[= dvorak technique =, . development of the ob...</td>\n",
       "      <td>Vernon Dvorak.</td>\n",
       "      <td>[Vernon Dvorak]</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.986971</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When did No. 20 Squadron relocate to Cairns?</td>\n",
       "      <td>[. now comprising 252 officers and men, the sq...</td>\n",
       "      <td>November 11, 1942.</td>\n",
       "      <td>[11 November 1942.]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.952935</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.979184</td>\n",
       "      <td>0.181818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Who won the Claxton Shield's Most Valuable Pla...</td>\n",
       "      <td>[. at the baseball australia diamond awards, h...</td>\n",
       "      <td>Wayne Lundgren.</td>\n",
       "      <td>[Wayne Lundgren.]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.844997</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Q: Who did Lesnar attack after Rollins refused...</td>\n",
       "      <td>[. lesnar won the match and his third wwe cham...</td>\n",
       "      <td>The New Day, the League of Nations, and Kevin ...</td>\n",
       "      <td>[A: Booker T, John \"Bradshaw\" Layfield, Michae...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.887968</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.812121</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Who is the ghost in the poem \"Little Gidding\"?</td>\n",
       "      <td>[. within the poem, the narrator meets a ghost...</td>\n",
       "      <td>The ghost represents a combination of various ...</td>\n",
       "      <td>[Combination of literary figures.]</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.918157</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.879476</td>\n",
       "      <td>0.181818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>Who did Brock Lesnar defeat to become the new ...</td>\n",
       "      <td>[. with the victory, lesnar became the undispu...</td>\n",
       "      <td>Shane Carwin.</td>\n",
       "      <td>[Randy Couture.]</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.982244</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.880024</td>\n",
       "      <td>0.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>When did Typhoon Kujira occur?</td>\n",
       "      <td>[. however, ultimately any effects in the arch...</td>\n",
       "      <td>April.</td>\n",
       "      <td>[April.]</td>\n",
       "      <td>0.638889</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.926135</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>What is the main sport in Manila?</td>\n",
       "      <td>[sports in manila have a long and distinguishe...</td>\n",
       "      <td>Basketball.</td>\n",
       "      <td>[Basketball.]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>Who ordered No. 202 Squadron RAF to Gibraltar?</td>\n",
       "      <td>[. so at 09 : 00 ( utc ) on the 9 september 19...</td>\n",
       "      <td>The person who gave the order is not mentioned...</td>\n",
       "      <td>[Admiralty]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.786328</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>What did soldiers build inside the Rock of Gib...</td>\n",
       "      <td>[work in gibraltar began immediately under com...</td>\n",
       "      <td>Tunnels and chambers.</td>\n",
       "      <td>[Tunnels and chambers.]</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.982325</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>86 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             question  \\\n",
       "0                 Who developed the Dvorak technique?   \n",
       "1        When did No. 20 Squadron relocate to Cairns?   \n",
       "2   Who won the Claxton Shield's Most Valuable Pla...   \n",
       "3   Q: Who did Lesnar attack after Rollins refused...   \n",
       "4      Who is the ghost in the poem \"Little Gidding\"?   \n",
       "..                                                ...   \n",
       "81  Who did Brock Lesnar defeat to become the new ...   \n",
       "82                     When did Typhoon Kujira occur?   \n",
       "83                  What is the main sport in Manila?   \n",
       "84     Who ordered No. 202 Squadron RAF to Gibraltar?   \n",
       "85  What did soldiers build inside the Rock of Gib...   \n",
       "\n",
       "                                             contexts  \\\n",
       "0   [= dvorak technique =, . development of the ob...   \n",
       "1   [. now comprising 252 officers and men, the sq...   \n",
       "2   [. at the baseball australia diamond awards, h...   \n",
       "3   [. lesnar won the match and his third wwe cham...   \n",
       "4   [. within the poem, the narrator meets a ghost...   \n",
       "..                                                ...   \n",
       "81  [. with the victory, lesnar became the undispu...   \n",
       "82  [. however, ultimately any effects in the arch...   \n",
       "83  [sports in manila have a long and distinguishe...   \n",
       "84  [. so at 09 : 00 ( utc ) on the 9 september 19...   \n",
       "85  [work in gibraltar began immediately under com...   \n",
       "\n",
       "                                               answer  \\\n",
       "0                                      Vernon Dvorak.   \n",
       "1                                  November 11, 1942.   \n",
       "2                                     Wayne Lundgren.   \n",
       "3   The New Day, the League of Nations, and Kevin ...   \n",
       "4   The ghost represents a combination of various ...   \n",
       "..                                                ...   \n",
       "81                                      Shane Carwin.   \n",
       "82                                             April.   \n",
       "83                                        Basketball.   \n",
       "84  The person who gave the order is not mentioned...   \n",
       "85                              Tunnels and chambers.   \n",
       "\n",
       "                                        ground_truths  context_precision  \\\n",
       "0                                     [Vernon Dvorak]           0.866667   \n",
       "1                                 [11 November 1942.]           1.000000   \n",
       "2                                   [Wayne Lundgren.]           1.000000   \n",
       "3   [A: Booker T, John \"Bradshaw\" Layfield, Michae...           0.000000   \n",
       "4                  [Combination of literary figures.]           0.200000   \n",
       "..                                                ...                ...   \n",
       "81                                   [Randy Couture.]           0.500000   \n",
       "82                                           [April.]           0.638889   \n",
       "83                                      [Basketball.]           1.000000   \n",
       "84                                        [Admiralty]           1.000000   \n",
       "85                            [Tunnels and chambers.]           0.500000   \n",
       "\n",
       "    faithfulness  answer_relevancy  context_recall  answer_similarity  \\\n",
       "0            1.0          1.000000             1.0           0.986971   \n",
       "1            1.0          0.952935             1.0           0.979184   \n",
       "2            1.0          0.844997             1.0           1.000000   \n",
       "3            0.0          0.887968             0.0           0.812121   \n",
       "4            1.0          0.918157             1.0           0.879476   \n",
       "..           ...               ...             ...                ...   \n",
       "81           1.0          0.982244             0.0           0.880024   \n",
       "82           1.0          0.926135             1.0           0.999999   \n",
       "83           1.0          1.000000             1.0           0.999999   \n",
       "84           0.0          0.000000             0.0           0.786328   \n",
       "85           1.0          0.982325             1.0           0.999999   \n",
       "\n",
       "    context_relevancy  \n",
       "0            0.125000  \n",
       "1            0.181818  \n",
       "2            0.000000  \n",
       "3            0.111111  \n",
       "4            0.181818  \n",
       "..                ...  \n",
       "81           0.428571  \n",
       "82           0.000000  \n",
       "83           0.300000  \n",
       "84           0.200000  \n",
       "85           0.111111  \n",
       "\n",
       "[86 rows x 10 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sudhanva/code/takehome/wandb/run-20240129_183034-zcxhwdfl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/prasadshreyas/wikitext-rag-eval/runs/zcxhwdfl' target=\"_blank\">noble-bush-7</a></strong> to <a href='https://wandb.ai/prasadshreyas/wikitext-rag-eval' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/prasadshreyas/wikitext-rag-eval' target=\"_blank\">https://wandb.ai/prasadshreyas/wikitext-rag-eval</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/prasadshreyas/wikitext-rag-eval/runs/zcxhwdfl' target=\"_blank\">https://wandb.ai/prasadshreyas/wikitext-rag-eval/runs/zcxhwdfl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faefa827a8c34ba88b90f17f98cf0393",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.003 MB of 0.020 MB uploaded\\r'), FloatProgress(value=0.15790969820438716, max=1.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>answer_relevancy</td><td>▁</td></tr><tr><td>answer_similarity</td><td>▁</td></tr><tr><td>context_precision</td><td>▁</td></tr><tr><td>context_recall</td><td>▁</td></tr><tr><td>context_relevancy</td><td>▁</td></tr><tr><td>faithfulness</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>answer_relevancy</td><td>0.85346</td></tr><tr><td>answer_similarity</td><td>0.89248</td></tr><tr><td>context_precision</td><td>0.57757</td></tr><tr><td>context_recall</td><td>0.67345</td></tr><tr><td>context_relevancy</td><td>0.16885</td></tr><tr><td>faithfulness</td><td>0.65116</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">noble-bush-7</strong> at: <a href='https://wandb.ai/prasadshreyas/wikitext-rag-eval/runs/zcxhwdfl' target=\"_blank\">https://wandb.ai/prasadshreyas/wikitext-rag-eval/runs/zcxhwdfl</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240129_183034-zcxhwdfl/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Logging to wandb\n",
    "\n",
    "# start a new wandb run to track this script\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"wikitext-rag-eval\",\n",
    "    \n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "        \"number_of_questions\": len(ds),\n",
    "        \"comments\": \"Compiled QA RAG model with teleprompter - OVERLAP 0\",\n",
    "        \"model\": \"RAG\",\n",
    "        \"dataset\": \"Synthetic\",\n",
    "        \"num_passages\": 5,\n",
    "        \"openai_model\": \"gpt-3.5-turbo\",\n",
    "        \"chroma_collection_name\": \"test\",\n",
    "        \"chroma_persist_directory\": \"chroma.db\",\n",
    "        \"chroma_local_embed_model\": \"sentence-transformers/paraphrase-MiniLM-L6-v2\",\n",
    "\n",
    "    }\n",
    ")\n",
    "\n",
    "wandb.log(result)\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No Retrieval\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicQA(dspy.Signature):\n",
    "    \"\"\"Answer questions with short factoid answers.\"\"\"\n",
    "\n",
    "    question = dspy.InputField()\n",
    "    answer = dspy.OutputField(desc=\"often between 1 and 5 words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the predictor.\n",
    "generate_answer = dspy.Predict(BasicQA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results_rows = []\n",
    "\n",
    "for index, row in test.iterrows():\n",
    "    # Get the question\n",
    "    question = row['question']\n",
    "    # Response from rag\n",
    "    response = generate_answer(question = question)\n",
    "    # Create a dictionary to represent a row\n",
    "    row_dict = {'question': question, 'answer': response.answer, 'ground_truths' : row['ground_truths']}\n",
    "    # Append the row dictionary to the list\n",
    "    eval_results_rows.append(row_dict)\n",
    "\n",
    "# Create the df_eval_results DataFrame from the list of rows\n",
    "df_eval_results = pd.DataFrame(eval_results_rows)\n",
    "\n",
    "# Convert 'ground_truths' column to list\n",
    "df_eval_results['ground_truths'] = df_eval_results['ground_truths'].apply(lambda x: ast.literal_eval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [answer_similarity]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:03<00:00,  1.59it/s]\n",
      "/home/sudhanva/miniconda3/lib/python3.11/site-packages/datasets/table.py:1395: FutureWarning: promote has been superseded by mode='default'.\n",
      "  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]\n",
      "/home/sudhanva/miniconda3/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    }
   ],
   "source": [
    "ds = Dataset.from_pandas(df_eval_results)\n",
    "\n",
    "result = evaluate(\n",
    "    ds,\n",
    "    metrics=[\n",
    "        answer_similarity\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer_similarity': 0.8535}"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_NAME = \"BASIC_QA_OVERLAP_64\"\n",
    "# save the result\n",
    "result.to_pandas().to_csv('./results/evaluation_' + EXP_NAME + '_' + TIMESTAMP + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sudhanva/miniconda3/lib/python3.11/site-packages/datasets/table.py:1395: FutureWarning: promote has been superseded by mode='default'.\n",
      "  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]\n",
      "/home/sudhanva/miniconda3/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>ground_truths</th>\n",
       "      <th>answer_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What type of teeth do temnospondyls have on th...</td>\n",
       "      <td>Conical teeth</td>\n",
       "      <td>[Tusks.]</td>\n",
       "      <td>0.815168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Who were the Principal Architects for Palestin...</td>\n",
       "      <td>Sir Ronald Storrs and Sir William Fisher</td>\n",
       "      <td>[Sir John James Burnet and Thomas Smith Tait.]</td>\n",
       "      <td>0.844589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the title of Brock Lesnar's autobiogra...</td>\n",
       "      <td>Death Clutch</td>\n",
       "      <td>[Death Clutch]</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Where is the replica of San Lorenzo Head 8 loc...</td>\n",
       "      <td>Museo Nacional de Antropología, Mexico City</td>\n",
       "      <td>[Utah Cultural Celebration Center.]</td>\n",
       "      <td>0.809204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What was the main flaw in the design of the Fu...</td>\n",
       "      <td>Weak armor</td>\n",
       "      <td>[Midships gun turrets.]</td>\n",
       "      <td>0.792137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>Q: How many Marines and Navy SEALs were part o...</td>\n",
       "      <td>A: 30</td>\n",
       "      <td>[A: 51 Marines and 9 Navy SEALs.]</td>\n",
       "      <td>0.803497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>What is the estimated weight of the La Cobata ...</td>\n",
       "      <td>Approximately 20 tons.</td>\n",
       "      <td>[40 tons.]</td>\n",
       "      <td>0.878407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>What is the name of the group of temnospondyls...</td>\n",
       "      <td>Metoposaurids</td>\n",
       "      <td>[Stereospondyli.]</td>\n",
       "      <td>0.824162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>Who promoted Brad Stevens to a full-time assis...</td>\n",
       "      <td>Doc Rivers</td>\n",
       "      <td>[Todd Lickliter.]</td>\n",
       "      <td>0.790983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>What is the population of Manila according to ...</td>\n",
       "      <td>1,780,148</td>\n",
       "      <td>[1,780,148.]</td>\n",
       "      <td>0.984600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>86 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             question  \\\n",
       "0   What type of teeth do temnospondyls have on th...   \n",
       "1   Who were the Principal Architects for Palestin...   \n",
       "2   What is the title of Brock Lesnar's autobiogra...   \n",
       "3   Where is the replica of San Lorenzo Head 8 loc...   \n",
       "4   What was the main flaw in the design of the Fu...   \n",
       "..                                                ...   \n",
       "81  Q: How many Marines and Navy SEALs were part o...   \n",
       "82  What is the estimated weight of the La Cobata ...   \n",
       "83  What is the name of the group of temnospondyls...   \n",
       "84  Who promoted Brad Stevens to a full-time assis...   \n",
       "85  What is the population of Manila according to ...   \n",
       "\n",
       "                                         answer  \\\n",
       "0                                 Conical teeth   \n",
       "1      Sir Ronald Storrs and Sir William Fisher   \n",
       "2                                  Death Clutch   \n",
       "3   Museo Nacional de Antropología, Mexico City   \n",
       "4                                    Weak armor   \n",
       "..                                          ...   \n",
       "81                                        A: 30   \n",
       "82                       Approximately 20 tons.   \n",
       "83                                Metoposaurids   \n",
       "84                                   Doc Rivers   \n",
       "85                                    1,780,148   \n",
       "\n",
       "                                     ground_truths  answer_similarity  \n",
       "0                                         [Tusks.]           0.815168  \n",
       "1   [Sir John James Burnet and Thomas Smith Tait.]           0.844589  \n",
       "2                                   [Death Clutch]           1.000000  \n",
       "3              [Utah Cultural Celebration Center.]           0.809204  \n",
       "4                          [Midships gun turrets.]           0.792137  \n",
       "..                                             ...                ...  \n",
       "81               [A: 51 Marines and 9 Navy SEALs.]           0.803497  \n",
       "82                                      [40 tons.]           0.878407  \n",
       "83                               [Stereospondyli.]           0.824162  \n",
       "84                               [Todd Lickliter.]           0.790983  \n",
       "85                                    [1,780,148.]           0.984600  \n",
       "\n",
       "[86 rows x 4 columns]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sudhanva/code/takehome/wandb/run-20240129_175403-d05oa4z7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/prasadshreyas/wikitext-rag-eval/runs/d05oa4z7' target=\"_blank\">balmy-puddle-3</a></strong> to <a href='https://wandb.ai/prasadshreyas/wikitext-rag-eval' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/prasadshreyas/wikitext-rag-eval' target=\"_blank\">https://wandb.ai/prasadshreyas/wikitext-rag-eval</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/prasadshreyas/wikitext-rag-eval/runs/d05oa4z7' target=\"_blank\">https://wandb.ai/prasadshreyas/wikitext-rag-eval/runs/d05oa4z7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cef690affacc401e839cd73683f42f69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.020 MB of 0.020 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>answer_similarity</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>answer_similarity</td><td>0.85353</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">balmy-puddle-3</strong> at: <a href='https://wandb.ai/prasadshreyas/wikitext-rag-eval/runs/d05oa4z7' target=\"_blank\">https://wandb.ai/prasadshreyas/wikitext-rag-eval/runs/d05oa4z7</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240129_175403-d05oa4z7/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Logging to wandb\n",
    "\n",
    "# start a new wandb run to track this script\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"wikitext-rag-eval\",\n",
    "    \n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "        \"number_of_questions\": len(ds),\n",
    "        \"comments\": \"No RAG model - just basic QA model - OVERLAP 64\",\n",
    "        \"model\": \"RAG\",\n",
    "        \"dataset\": \"Synthetic\",\n",
    "        \"num_passages\": 5,\n",
    "        \"openai_model\": \"gpt-3.5-turbo\",\n",
    "        \"chroma_collection_name\": \"test\",\n",
    "        \"chroma_persist_directory\": \"chroma.db\",\n",
    "        \"chroma_local_embed_model\": \"sentence-transformers/paraphrase-MiniLM-L6-v2\",\n",
    "\n",
    "    }\n",
    ")\n",
    "\n",
    "wandb.log(result)\n",
    "\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
